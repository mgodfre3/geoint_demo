apiVersion: apps/v1
kind: Deployment
metadata:
  name: analyst-assistant
  namespace: geoint-assistant
  labels:
    app: analyst-assistant
spec:
  replicas: 1
  selector:
    matchLabels:
      app: analyst-assistant
  template:
    metadata:
      labels:
        app: analyst-assistant
    spec:
      containers:
        # Ollama â€” local LLM inference (replaces Foundry Local until container support ships)
        - name: ollama
          image: ollama/ollama:latest
          ports:
            - containerPort: 11434
          resources:
            limits:
              nvidia.com/gpu: "1"
              memory: "8Gi"
            requests:
              memory: "4Gi"
          volumeMounts:
            - name: ollama-data
              mountPath: /root/.ollama

        # ChromaDB vector store
        - name: chromadb
          image: chromadb/chroma:latest
          ports:
            - containerPort: 8000
          resources:
            limits:
              memory: "2Gi"
            requests:
              memory: "1Gi"
          volumeMounts:
            - name: chroma-data
              mountPath: /chroma/chroma

        # RAG backend + Chat API
        - name: chat-backend
          image: acrgeointdemo.azurecr.io/geoint/chat-api:latest
          ports:
            - containerPort: 8087
          resources:
            limits:
              memory: "2Gi"
            requests:
              memory: "1Gi"
          env:
            - name: FOUNDRY_URL
              value: "http://localhost:11434"
            - name: CHROMA_URL
              value: "http://localhost:8000"
            - name: VISION_API_URL
              value: "http://vision-pipeline.geoint-vision.svc:8082"

      volumes:
        - name: ollama-data
          emptyDir: {}
        - name: chroma-data
          emptyDir: {}
